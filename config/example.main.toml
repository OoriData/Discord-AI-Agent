[b4a] # Config for e.g. tools & resources, perhaps via MCP

source_sets = [
  "toys.b4a.toml"
  # "news.b4a.toml"
  # "gdrive_mcp.b4a.toml",
]

[llm_endpoint]
# label = "Local LM Studio"
base_url = "http://localhost:1234/v1"
api_key = "lm-studio"

# Use this form if you're using LM Studio to run mlx-community/Qwen2.5-14B-Instruct-1M-4bit
# model = "qwen2.5-14b-instruct-1m"
# Or use this if you're running the same model with mlx_lm.server
model = "mlx-community/Qwen2.5-14B-Instruct-1M-4bit"

# mlx-community/Llama-3.2-3B-Instruct-4bit is very small & efficient, but not great at reasoning for tool-calling
# model = "llama-3.2-3b-instruct"  # LM Studio form
# model = "mlx-community/Llama-3.2-3B-Instruct-4bit"  # mlx_lm.server form

sysmsg = "You are a helpful AI assistant running locally via LM Studio. You can access tools using MCP servers."
# sys_postscript = "Respond in a helpful and concise manner."

[model_params]
temperature = 0.3
max_tokens = 1000
