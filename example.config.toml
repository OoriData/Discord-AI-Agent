[b4a] # Config for e.g. tools & resources, perhaps via MCP

sources = [
  "gdrive_mcp.b4a.toml",
  "news.b4a.toml"
]

[llm_endpoint]
base_url = "http://localhost:1234/v1"
api_key = "lm-studio"
label = "Local LM Studio"
sysmsg = "You are a helpful AI assistant running locally via LM Studio. You can access tools using MCP servers."
sys_postscript = "Respond in a helpful and concise manner."

[model_params]
temperature = 0.3
max_tokens = 1000
